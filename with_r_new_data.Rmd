---
title: "better_data"
author: "al"
date: "06/04/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

## Prepare for analysis

```{r}
rm(list=ls())
graphics.off()
gc()
setwd("~/projects/car_data_analysis")
library(ggplot2)
library(dplyr)
library(plotly)
set.seed(123)

```

## Load in data, basic transformations and missingness checks
```{r}
a <- read.csv('newly_scraped.csv', stringsAsFactors=T)
a <- unique(a)
names(a)[2] <- "age"
a["age"] <- 2021-a["age"]

summary(is.na(a))
a <- a[!is.na(a$mileage),]
a <- a[!is.na(a$age),]
a <- a[!is.na(a$engine.size),]
```


## Train/test splits and other investigative transformations
```{r}
dt = sort(sample(nrow(a), nrow(a)*.7))
train<-a[dt,]
test<-a[-dt,]

str(train)
str(test)

enforce_make_consistency <- function(tr=train, te=test, print_removals=T){
  train_makes <- table(tr$make)
  test_makes <- table(te$make)

  
  # Remove from train set if it has <2 obvs of make (can't generate a line?)
  row_mask <- grepl(paste(names(train_makes[train_makes<2]), collapse='|'),tr$make)
  tr <- tr[!row_mask,]
  if(print_removals){
    print("Train row removals")
    print(summary(row_mask))
  }
  
  
  # Remove from test set if make didn't appear in train set
  row_mask <- grepl(paste(names(train_makes[train_makes==0]), collapse='|'),te$make)
  te <- te[!row_mask,]
  if(print_removals){
    print("Test row removals")
    print(summary(row_mask))    
  }

  
  return(list(tr,te))
}

d <- enforce_make_consistency()
train <- d[[1]]
test <- d[[2]]
rm(d)

str(train)
str(test)

freq_by_make <- as.data.frame(table(train$make))
names(freq_by_make)[1] <- "make"
price_by_make <- a %>% 
  group_by(make) %>% 
  summarise(m=mean(price))
overall_mean <- mean(train$price)
```


## Look at 1D relationships
```{r}
# locater
plot(price~mileage, data=a)
plot(price~engine.size, data=a)
na.owners.as.neg <- a
na.owners.as.neg[is.na(na.owners.as.neg)] <- -1
boxplot(price~owners, data=na.owners.as.neg) # Boxplots better
plot(price~age, data=a)

g <- ggplot(data=freq_by_make, aes(x=make, col="red")) +
  scale_x_discrete(guide = guide_axis(angle = 90)) +
  geom_col(data=price_by_make, aes(y=m, col="blue", alpha=0.4)) +
  geom_hline(yintercept = overall_mean) +
  ylab("mean price") +
  geom_col(data=freq_by_make, aes(y=Freq*10, alpha=0.4)) +
  scale_y_continuous(sec.axis=sec_axis(trans=~./100, name="freq") ) +
  scale_color_discrete(labels=c("price","freq"))
g
# Smarter way to check for freq vs mean price of make
```

## Fit models and compare via cross-validation

### Helper functions
```{r}
evaluate <- function(model, evaluation_data=test){
  return(
    list(mean_residual=mean(evaluation_data$price - predict(model, evaluation_data, type="response")),
         mean_abs=mean(abs(evaluation_data$price - predict(model, evaluation_data, type="response"))),
         mean_sq=mean((evaluation_data$price - predict(model, evaluation_data, type="response"))^2)
         )
  )
}

extract_covariates <- function(d, predictor="price") {
  d <- tibble::rownames_to_column(d, "observation")
  responses = d[colnames(d) != predictor ]
  return(
    split(responses, seq(nrow(responses)) )
    )
}

confusion_plot <- function(model, d){
  return(
    ggplot(data=d) +
      geom_point(aes(
              x=price, y=predict(model,d, type="response"), 
              text=extract_covariates(d)
                )
             )
  )
}

"%notin%" <- function(x, table) match(x, table, nomatch = 0) == 0

```


### Mileage and outlier removal
```{r}
mileage0 <- lm(price~mileage, data=train)
plot(mileage0, ask=F)
p0 <- confusion_plot(mileage0, train)
ggplotly(p0, tooltip="text")
# Plots show subtle signs of heteroskedasticity (high values have more variance)
# TODO: fit with a varying assumption on the variance
# Use plots to identify outliers
outliers <- c("779","10893", "7554", "5339", "1758", "5474", "1553")

train <- subset(train, rownames(train) %notin% outliers)
d <- enforce_make_consistency()
train <- d[[1]]
test <- d[[2]]
rm(d)

mileage1 <- lm(price~mileage, data=train)
plot(mileage1, ask=F)
cp0 <- confusion_plot(mileage1, train)
ggplotly(cp0, tooltip="text")
anova(mileage0)

# Verify removing outliers helped in at least the mileage fit
evaluate(mileage0)
evaluate(mileage1)
# Helps a little
```

### Scaling
```{r}
# Verify that scaling the inputs doesnt change the OLS fit of a linear model
# (mathematically should not, therefore any change is truncation error)
my.scale <- function(df, col){
  q <- scale(df[col], center=T)
  scaled <- df
  scaled[col] <- as.vector(q)
  return(scaled)
}
train_scaled <- my.scale(train, "mileage")
test_scaled <- my.scale(test, "mileage")


scaled.mileage0 <- lm(price~mileage, data=train_scaled)
plot(scaled.mileage0, ask=F)
sp0 <- confusion_plot(scaled.mileage0, train_scaled)
ggplotly(sp0, tooltip="text")

evaluate(scaled.mileage0, evaluation_data = test_scaled)
evaluate(mileage1)
# Performs only marginally worse - not sure we need to scale input right now
# anyway, could be more interesting/relevant when in context of training
# dynamics of a gradient descent method and how the resolution of the scale
# affects training due to practicalities of truncation error
```

### Make
```{r}
make1 <- lm(price~make, data=train)
plot(make1, ask=F)
# Intuition: High dimensionality means there is more likely to be outliers (?)
# Clearly heteroskedastic with high variance with large fits
# Could be due to low quantity of estimates in the lower price ranges

p1 <- confusion_plot(make1, train)
ggplotly(p1, tooltip="text")

# Unclear what is benefit of anova here in context of prediction instead of
# interpretation(?)
anova(make1)

# Check which on it's own has better predictive power
evaluate(make1)
evaluate(mileage1)
# Mileage is the winner
```

### Make as mean
```{r}
# What if we replace make with average price of car make:
# For a simple additive fit we expect no change
get.mean.make.price <- function(row){
  return(
    # 6th col of our dataframe is make
    # We want to get the 2nd col from price_by_make which is the mean price
    as.numeric(price_by_make[ price_by_make$make == row[6], ][2])
  )
}

adapt_make <- function(df){
  adapted_df <- df
  adapted_df["make"] <- apply(df,1,get.mean.make.price)
  return(adapted_df)
}

adapted_a <- adapt_make(a)
# Another random plot that is also indicative that more expensive cars 
# have a greater variance
plot(price~make, data=adapted_a)

adapted_train <- adapt_make(train)
adapted_test <- adapt_make(test)

adapted.make1 <- lm(price~make, data=adapted_train)
# TODO: Understand why residuals vs leverage plot changes
# TODO: Understand why the confusion pots also changed :thinking_face:
# Something to do with combining effects of cars that are similarly priced?
plot(adapted.make1, ask=F)
ap1 <- confusion_plot(adapted.make1, adapted_train)
ggplotly(ap1, tooltip="text")
anova(adapted.make1)

# Adapting seems to marginally improve predictive performance, not sure why
evaluate(adapted.make1, evaluation_data=adapted_test)
evaluate(make1)
```

### Make and mileage
```{r}
make.dot.mileage1 <- lm(price~make*mileage, data=train)
plot(make.dot.mileage1, ask=F)
outliers <- c("5471", "3283")
# Not doing anything with outliers yet
# Problematic leverages
# Intuition is that joint distributions have a low occupancy

p2 <- confusion_plot(make.dot.mileage1,train)
ggplotly(p2, tooltip="text")

make.add.mileage1 <- lm(price~make+mileage, data=train)
plot(make.add.mileage1, ask=F)
# Scale location seems a bit curvilinear and starting to clearly suffer
# from a constraint on mapping to R+
bp2 <- confusion_plot(make.add.mileage1,train)
ggplotly(bp2, tooltip="text")


# Don't need to worry about weird occupancy and rank issues from above as 
# add is outperforming the dot
evaluate(make.add.mileage1)
evaluate(make.dot.mileage1)
```

### Make as mean and mileage
```{r}

make.as.mean.dot.mileage <- lm(price~make*mileage, data=adapted_train)
plot(make.as.mean.dot.mileage, ask=F)
# No significant improvement apart from nicer leverage plot
ap2 <- confusion_plot(make.as.mean.dot.mileage ,adapted_train)
ggplotly(ap2, tooltip="text")

make.as.mean.add.mileage <- lm(price~make+mileage, data=adapted_train)
plot(make.as.mean.add.mileage, ask=F)
bap2 <- confusion_plot(make.as.mean.add.mileage,adapted_train)
ggplotly(bap2, tooltip="text")

evaluate(make.dot.mileage1)
# This model interestingly performs the best :thinking_face:
# TODO: Ponder on the maths of changing factor into a subgroup mean
# Clearly reduces dimensionality/something to do with occupancy somehow
evaluate(make.as.mean.dot.mileage, evaluation_data = adapted_test)

# Additively though it seems to outperform with the factors
evaluate(make.add.mileage1)
evaluate(make.as.mean.add.mileage, evaluation_data = adapted_test)
```

### Make, age and mileage
```{r}
make.add.age.add.mileage <- lm(price~make+age+mileage, data=train)
# Introduction of more curvilinear response of residual on fitted:
plot(make.add.age.add.mileage, ask=F)

# Best model yet!
evaluate(make.add.age.add.mileage)
evaluate(make.add.mileage1)
```

### Do it all additively and look into imputation
```{r}
# Need to think about prediction intervals on response
# - Introduce reject options
# - Evaluate prediction intervals via bootstrapping
# - Can use diagnostically:
#   - Width clustering with respect space can give info on low occupancy
# Intervals on the MSE
# - To compare results more confidently
# Remap price onto categories

# Initially we can't include owners as it is missing data
all.but.owners <- lm(price~make+age+mileage+engine.size, data=train)
evaluate(all.but.owners)
ggplotly(confusion_plot(all.but.owners,train))

# Impute owners with average and try with everything
with.owners <- train[!is.na(train$owners),]
without.owners <- train[is.na(train$owners),]
mean_owner <- mean(with.owners$owners)
without.owners$owners <- mean_owner
imputed.mean.owners.train <- rbind(with.owners, without.owners)

with.owners <- test[!is.na(test$owners),]
without.owners <- test[is.na(test$owners),]
mean_owner <- mean(with.owners$owners)
without.owners$owners <- mean_owner
imputed.mean.owners.test <- rbind(with.owners, without.owners)

all.mean.owners <- lm(price~., data=imputed.mean.owners.train)
# Not a significant improvement at all
evaluate(all.mean.owners, evaluation_data = imputed.mean.owners.test)

# TODO: below
# Impute owners with sub-group average first and global average second
# then try with everything
best_model<-all.mean.owners
best_eval<-imputed.mean.owners.test
best_train<-imputed.mean.owners.train
```


### Try a GLM
```{r}
gamma.all.but.owners.or.make <- glm(
  price~age+mileage+engine.size, 
  data=train, 
  family=Gamma(link="log")
  )
poi.all.but.owners.or.make <- glm(
  price~age+mileage+engine.size, 
  data=train, 
  family=poisson(link="log")
  )
norm.transformed <- glm(
  price~age+mileage+engine.size, 
  data=train, 
  family=gaussian(link="log")
  )

to_beat <- evaluate(best_model, evaluation_data = best_eval)
evaluate(gamma.all.but.owners.or.make)
evaluate(poi.all.but.owners.or.make)
evaluate(norm.transformed)

confusion_plot(best_model, best_train)
ggplotly(confusion_plot(poi.all.but.owners.or.make, train))
confusion_plot(norm.transformed, train)
ggplotly(confusion_plot(gamma.all.but.owners.or.make, train))
```

### Remove GLM outliers
```{r}
nt <- train[abs(poi.all.but.owners.or.make$residuals) < 2*mean(poi.all.but.owners.or.make$residuals)*train$price/1000,]
outliers <- c("10911", "11119")

cleaner_train <- subset(nt, rownames(train) %notin% outliers)
d <- enforce_make_consistency(tr=cleaner_train)
cleaner_train <- d[[1]]
cleaner_test <- d[[2]]
rm(d)

c.gamma.all.but.owners.or.make <- glm(
  price~age+mileage+engine.size, 
  data=cleaner_train, 
  family=Gamma(link="log")
  )
c.poi.all.but.owners.or.make <- glm(
  price~age+mileage+engine.size, 
  data=cleaner_train, 
  family=poisson(link="log")
  )
c.norm.transformed <- glm(
  price~age+mileage+engine.size, 
  data=cleaner_train, 
  family=gaussian(link="log")
  )

# Look into how predictive outliers impact MAE vs MSE
# Need to start bootstrapping for intervals/or doing stats tests
# Think to look at distributions of predictions ?!
# hists with same axes to compare

evaluate(c.gamma.all.but.owners.or.make, evaluation_data = cleaner_test)
evaluate(c.poi.all.but.owners.or.make, evaluation_data = cleaner_test)
evaluate(c.norm.transformed, evaluation_data = cleaner_test)

confusion_plot(all.but.owners, train)
ggplotly(confusion_plot(c.poi.all.but.owners.or.make, cleaner_train))
confusion_plot(c.norm.transformed, cleaner_train)
ggplotly(confusion_plot(c.gamma.all.but.owners.or.make, cleaner_train))

# Keep thinking about outliers
plot(c.poi.all.but.owners.or.make)

# Something about age doesnt feel right
## Look into effect on residuals
cleaner_test$residuals <- cleaner_test$price - predict(c.poi.all.but.owners.or.make, cleaner_test, type="response")
plot(cleaner_test$age, cleaner_test$residuals)
plot(cleaner_test$mileage, cleaner_test$residuals)
plot(cleaner_test$engine.size, cleaner_test$residuals)


# This does not improve what we are witnessing
# - how to deal with heteroskedascity?!?!
c.poi.all.but.owners.or.make.with.age.interaction <- glm(
  price~age*mileage+age*engine.size, 
  data=cleaner_train, 
  family=poisson(link="log")
  )

evaluate(c.poi.all.but.owners.or.make.with.age.interaction, evaluation_data = cleaner_test)
confusion_plot(c.poi.all.but.owners.or.make.with.age.interaction, cleaner_train)
cleaner_test$other_residuals <- cleaner_test$price - predict(c.poi.all.but.owners.or.make.with.age.interaction, cleaner_test, type="response")
plot(cleaner_test$age, cleaner_test$other_residuals)
plot(cleaner_test$mileage, cleaner_test$other_residuals)
plot(cleaner_test$engine.size, cleaner_test$other_residuals)

# Look at Cook's distances
cdists <- cooks.distance(c.poi.all.but.owners.or.make)
hist(cdists)
cdists[cdists > 10]

outliers = c("1944", "2198", "3351", "3784", "5387", "12514")
leave.one.out = list()

for(to_remove in seq(5)) {
  o <- combn(outliers, to_remove)
  for(i in seq(ncol(o)) ) {
    cleaner_train <- subset(train, rownames(train) %notin% o[,i])
    d <- enforce_make_consistency(tr=cleaner_train, print_removals=F)
    cleaner_train <- d[[1]]
    cleaner_test <- d[[2]]
    rm(d)
    
    m <-  glm(price~age+mileage+engine.size, 
              data=cleaner_train, 
              family=poisson(link="log")
              )
    evaluations <- evaluate(m, evaluation_data = cleaner_test)
    missing_vars <- o[,i]
    leave.one.out[length(leave.one.out)+1] <- list(append(evaluations, list(vars=missing_vars)))
  }
}

# Get better models than best
best_model = all.but.owners
best_result = evaluate(best_model)
better_models = list()
for(result in leave.one.out){
  if(result$mean_sq < best_result$mean_sq){
      better_models[length(better_models) + 1] <- result
  }
}


```

### Outlier removal in best model

```{r}
plot(best_model)
# Look at Cook's distances
cdists <- cooks.distance(best_model)
hist(cdists, breaks="FD")
cutoff <- 0.2
hist(cdists[cdists<cutoff], breaks="FD")
cooks_outliers <- cdists[cdists > cutoff]
outliers <- c("3351", "6783", "12514")
outliers <- c(outliers, names(cooks_outliers))

# Get better models than best
best_model = all.but.owners
best_model$call
best_result = list(mean_sq=Inf)
best.leave.one.out = list()


for(to_remove in seq(2)) {
  o <- combn(outliers, to_remove)
  print(length(o))
  for(i in seq(ncol(o)) ) {
    cleaner_train <- subset(train, rownames(train) %notin% o[,i])
    d <- enforce_make_consistency(tr=cleaner_train, print_removals = F)
    cleaner_train <- d[[1]]
    cleaner_test <- d[[2]]
    rm(d)
    
    m <-  lm(formula = price ~ make + age + mileage + engine.size, data = cleaner_train)
    evaluations <- evaluate(m, evaluation_data = cleaner_test)
    missing_vars <- o[,i]
    if(evaluations$mean_sq < best_result$mean_sq){
      # best_result <- evaluations
      print("*")
      best.leave.one.out[length(best.leave.one.out)+1] <- list(append(evaluations, list(vars=missing_vars)))
    }
  }
}

```

### nnet non-linear investigation (crude!)

```{r}
d <- na.omit(train)
nnet(price~., data=d, size=15, decay=0.1)
x <- nnet(price~., data=d, size=15, decay=0.1)
x <- nnet(price~., data=d, size=2, decay=0.1)
e <- scale(d)
d <- my.scale(d, "mileage")
d <- my.scale(d, "engine.size")
d <- my.scale(d, "age")
View(d)
View(d)
d <- my.scale(d, "owners")
y <- nnet(price~., data=d, size=2, decay=1e-3)
y <- nnet(price~., data=d, size=100, decay=1e-3)
y <- nnet(price~., data=d, size=15, decay=1e-3)

```



### Investigating PCA
```{r}
train_no_na <- a[c("mileage", "age", "engine.size", "owners")] %>% 
  filter(across(everything(),
                ~ !is.na(.)))
# Let's standardize our components so comparison is valid
# Misunderstood PCA for what is and isnt relevant to the fit
means <- apply(train_no_na,2,mean)
stds <- apply(train_no_na,2,sd)
stnn <- scale(train_no_na)

pca <- prcomp(stnn)
biplot(pca)
# Suggests owners about as useful as age?
```